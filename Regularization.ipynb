{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary size = 30K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionGeneration():\n",
    "    def __init__(self, lm_gq, uqa_qg, regularization):\n",
    "        self.lm_qg = lm_qg\n",
    "        self.uqa_qg = uqa_qg\n",
    "        self.regularization = regularization\n",
    "        \n",
    "    def generate_token(self, context: str, ans: str, list_question_tokens: list) -> str:\n",
    "        '''\n",
    "        Input:\n",
    "            - context:\n",
    "            - ans:\n",
    "            - list_question_tokens: the history of generated question tokens.\n",
    "            The form of list_question_tokens is a list of (q_token, class), where class is 0 for lm\n",
    "            token and 1 for uqa token\n",
    "        Output:\n",
    "            - The next generated question token\n",
    "        '''\n",
    "        # contains the tokens of the generated question\n",
    "        list_question_tokens = []\n",
    "        # contains the classes of each token of the gen. question. Same len as list_question_tokens\n",
    "        # 0 = LM, 1 = UQA\n",
    "        list_token_classes = []\n",
    "        qi = None\n",
    "        \n",
    "        while not self.finished_generation(qi):\n",
    "            # Generate the probs of the ith query token using the lm and uqa models\n",
    "            lm_qi_probs = self.lm_qg.forward(context, ans, list_question_tokens)\n",
    "            uqa_qi_probs = self.uqa_qg.forward(context, ans, list_question_tokens)\n",
    "\n",
    "            # Get the final token  prob distribution using regulatization\n",
    "            qi_probs, qi_class = self.regularization(lm_qi_probs, uqa_qi_probs, \n",
    "                                                     list_question_tokens, list_token_classes)\n",
    "\n",
    "            # Get the predicted token\n",
    "            qi = torch.argmax(qi_probs, dim=1)\n",
    "            list_question_tokens.append(vocab_idx2str[qi])\n",
    "            list_token_classes.append(qi_class)\n",
    "            \n",
    "            \n",
    "    def finished_generation(self, question_token):\n",
    "        return question_token == '?' or question_token =='EOS'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis)",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
