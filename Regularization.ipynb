{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T06:21:03.766174Z",
     "start_time": "2020-05-11T06:21:03.707702Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Freq_Regularization():\n",
    "    def predict_class(self, list_token_classes):\n",
    "        '''\n",
    "        UQA class = 0\n",
    "        LM class = 1\n",
    "        '''\n",
    "        if len(list_token_classes) == 0:\n",
    "            return np.random.choice(2, 1, p=[0.5, 0.5])[0]\n",
    "\n",
    "        prob_uqa = sum(list_token_classes)/len(list_token_classes)\n",
    "        prob_lm = 1 - prob_uqa\n",
    "        return np.random.choice(2, 1, p=[prob_uqa, prob_lm])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T06:21:04.459967Z",
     "start_time": "2020-05-11T06:21:04.077268Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from BERTQG.token_generation import load_model, generate_token\n",
    "#from Regularization_module import Regularizer_Discriminator\n",
    "from itertools import groupby\n",
    "\n",
    "list_wh = ['what', 'which', 'where', 'when', 'who', 'why', 'how', 'whom', 'whose']\n",
    "\n",
    "class QuestionGeneration():\n",
    "    def __init__(self, bert_model, lm_gq, uqa_qg, regularization=None):\n",
    "        self.lm_qg, _, _ = load_model(bert_model, lm_qg)\n",
    "        self.uqa_qg, self.tokenizer, self.device = load_model(bert_model, uqa_qg)\n",
    "        self.regularization = Freq_Regularization()\n",
    "        \n",
    "    def generate_question(self, context: str, ans: str, ans_start: str, list_question_tokens: list = []) -> str:\n",
    "        '''\n",
    "        Input:\n",
    "            - context\n",
    "            - ans\n",
    "            - ans_start\n",
    "            - list_question_tokens: the history of generated question tokens.\n",
    "            The form of list_question_tokens is a list of (q_token, class), where class is 0 for uqa\n",
    "            token and 1 for lm token\n",
    "        Output:\n",
    "            - The next generated question token\n",
    "        '''\n",
    "        # contains the classes of each token of the gen. question. Same len as list_question_tokens\n",
    "        # 0 = UQA, 1 = LM\n",
    "        list_token_classes = []\n",
    "        qi = None\n",
    "        \n",
    "        list_banned_verbs = []\n",
    "        list_question_failures = []\n",
    "        \n",
    "        # generation finished when [SEP] is created\n",
    "        while not self.__finished_generation(qi):\n",
    "            question_text = \" \".join(list_question_tokens)\n",
    "            \n",
    "            # Generate the tokens and probs of the ith query token using the lm and uqa models\n",
    "            lm_qi_token, lm_qi_idx, lm_qi_probs = generate_token(self.lm_qg, self.tokenizer, self.device, context, question_text, ans, ans_start)\n",
    "            uqa_qi_token, uqa_qi_idx, uqa_qi_probs = generate_token(self.uqa_qg, self.tokenizer, self.device, context, question_text, ans, ans_start)\n",
    "            \n",
    "            print(\"lm tokens\", self.__get_topk_tokens(lm_qi_probs))\n",
    "            print(\"uqa_qi_probs\", self.__get_topk_tokens(uqa_qi_probs))\n",
    "            \n",
    "            # Check if the question is corrupted\n",
    "            \n",
    "            list_lm_tokens = self.__get_topk_tokens(lm_qi_probs)\n",
    "            list_uqa_tokens = self.__get_topk_tokens(uqa_qi_probs)\n",
    "            corrupted_question = self.__corrupted_question(list_question_tokens[-1], list_lm_tokens, list_uqa_tokens)\n",
    "            if corrupted_question:\n",
    "                print(\"Restarting question!!!!\")\n",
    "                list_banned_verbs.append(list_question_tokens[1])\n",
    "                # restart the question with a new verb becasue we reached a corrupted question\n",
    "                list_question_tokens = [list_question_tokens[0]]\n",
    "                list_question_failures.append(question_text)\n",
    "                list_token_classes = []\n",
    "                continue\n",
    "            \n",
    "            # Get token class to use\n",
    "            qi_class = self.regularization.predict_class(list_token_classes)\n",
    "            \n",
    "            \n",
    "            # Get the predicted token\n",
    "            if qi_class == 1: # LM\n",
    "                # clean prob distrib of lm\n",
    "                lm_qi_token, lm_qi_probs = self.__clean_token_distrib(lm_qi_probs, list_question_tokens, list_banned_verbs)\n",
    "                qi = lm_qi_token\n",
    "            else: # UQA\n",
    "                #clean the prob distrib of uqa\n",
    "                uqa_qi_token, uqa_qi_probs = self.__clean_token_distrib(uqa_qi_probs, list_question_tokens, list_banned_verbs)\n",
    "                qi = uqa_qi_token\n",
    "                \n",
    "            print(\"qi_class\", qi_class)\n",
    "            print(\"################\")\n",
    "            list_question_tokens.append(qi)\n",
    "            list_token_classes.append(qi_class)\n",
    "            \n",
    "#             # for testing\n",
    "#             print(list_question_tokens)\n",
    "#             print([\"UQA\" if cls == 0 else \"LM\" for cls in list_token_classes])\n",
    "#             print(\"\\n\")\n",
    "        list_question_tokens = self.__remove_consecutive_repeated_tokens(list_question_tokens)\n",
    "        print(\"Attempted Questions:\", list_question_failures)\n",
    "        return \" \".join(list_question_tokens[:-1]) # without [SEP]\n",
    "            \n",
    "    def __finished_generation(self, question_token):\n",
    "        return question_token =='[SEP]'\n",
    "    \n",
    "    def __prob_distrib2tokens(self, probs):\n",
    "        return self.tokenizer.convert_ids_to_tokens(probs)\n",
    "    \n",
    "    def __get_topk_tokens(self, probs, topk=10):\n",
    "        '''\n",
    "        Input:\n",
    "            - probs: probability distribution over vocab (output distribution)\n",
    "            - topk: k to return\n",
    "        Output:\n",
    "            - list of tokens (top 1 to k)\n",
    "        '''\n",
    "        return self.tokenizer.convert_ids_to_tokens(reversed(np.argsort(probs.tolist())[-topk:]))\n",
    "\n",
    "    def __ignore_probs(self, probs, idx_list):\n",
    "        '''\n",
    "        Input:\n",
    "            - probs: probability distribution over vocab (output distribution)\n",
    "            - idx_list: index list to ignore (to force it as -inf)\n",
    "        Output:\n",
    "            - new probs where probabilities of some tokens are ignored\n",
    "        '''\n",
    "        for idx in idx_list:\n",
    "            probs[idx] = -float('inf')\n",
    "        return probs\n",
    "\n",
    "    def __clean_token_distrib(self, probs, list_tokens: list, list_banned_verbs: list):\n",
    "        wh_generated = any([True for token in list_tokens if token.lower() in list_wh])\n",
    "        list_to_remove = ['you', 'it', 'they', 'he', 'she', \"i\", 'we']\n",
    "        list_to_remove.extend(list_banned_verbs)\n",
    "        if wh_generated:\n",
    "            # if the wh word was already geneerated, do no generate more wh word\n",
    "            list_to_remove.extend(list_wh)\n",
    "        ignore_idx_list = self.tokenizer.convert_tokens_to_ids(list_to_remove)\n",
    "        probs = self.__ignore_probs(probs, ignore_idx_list)\n",
    "        list_topk_tokens = self.__get_topk_tokens(probs)\n",
    "        return list_topk_tokens[0], probs\n",
    "       \n",
    "        \n",
    "    def __corrupted_question(self, prev_token: str, list_lm_tokens: list, list_uqa_tokens: list):\n",
    "        '''\n",
    "        If the output distrib. are like:\n",
    "            lm tokens ['##n', '##ns', '##nn', '##na', '##ne', '##m', '##nne', '##nb', '##nt', '##nia']\n",
    "            uqa_qi_probs ['##n', '##ns', '##nh', '##ne', '##na', '##nst', '##s', '##no', '##ness', '##ng']\n",
    "        the question is corrupted and we should backtrack. Maybe a different verb is needed\n",
    "        '''\n",
    "        #take the top 10 tokens\n",
    "        list_lm_tokens = list_lm_tokens[:10]\n",
    "        list_uqa_tokens = list_uqa_tokens[:10]\n",
    "        \n",
    "        num_hash_lm = sum([1 for token in list_lm_tokens if \"##\" in token])\n",
    "        num_hash_uqa = sum([1 for token in list_uqa_tokens if \"##\" in token])\n",
    "        return num_hash_lm >= 7 and num_hash_uqa > 7 and prev_token in self.tokenizer.vocab\n",
    "        \n",
    "    def __remove_consecutive_repeated_tokens(self, list_tokens):\n",
    "        '''\n",
    "        Removes consecutive tokens.\n",
    "        Sometimes the generated question is \"when when did...\",\n",
    "        so we need to remove one when\n",
    "        '''\n",
    "        return [x[0] for x in groupby(list_tokens)]\n",
    "    #assert remove_consecutive_repeated_tokens([1,1,1,1,1,1,2,3,4,4,5,1,2]) == [1, 2, 3, 4, 5, 1, 2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T06:21:10.608658Z",
     "start_time": "2020-05-11T06:21:04.461405Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/11/2020 06:21:04 - INFO - BERTQG.tokenization -   loading vocabulary file BERTQG/models/bert-base-uncased/vocab.txt\n",
      "05/11/2020 06:21:06 - INFO - BERTQG.modeling -   loading archive file BERTQG/models/bert-base-uncased\n",
      "05/11/2020 06:21:06 - INFO - BERTQG.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 3,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load BERTQG/models/lm_10k_QG/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/11/2020 06:21:08 - INFO - BERTQG.tokenization -   loading vocabulary file BERTQG/models/bert-base-uncased/vocab.txt\n",
      "05/11/2020 06:21:08 - INFO - BERTQG.modeling -   loading archive file BERTQG/models/bert-base-uncased\n",
      "05/11/2020 06:21:08 - INFO - BERTQG.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 3,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load BERTQG/models/uqa_10k_QG/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "bert_model = 'BERTQG/models/bert-base-uncased'\n",
    "lm_qg = 'BERTQG/models/lm_10k_QG/pytorch_model.bin'\n",
    "uqa_qg = 'BERTQG/models/uqa_10k_QG/pytorch_model.bin'\n",
    "#regul_model = \"./discri_model.bin\"# model path here\n",
    "QG = QuestionGeneration(bert_model, lm_qg, uqa_qg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T06:21:10.612119Z",
     "start_time": "2020-05-11T06:21:10.610202Z"
    }
   },
   "outputs": [],
   "source": [
    "context = \"The black bittern (Ixobrychus flavicollis) is a bittern of Old World origin, breeding in tropical Asia from Bangladesh, Pakistan, India, and Sri Lanka east to China, Indonesia, and Australia. It is mainly resident, but some northern birds migrate short distances.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T06:21:10.718339Z",
     "start_time": "2020-05-11T06:21:10.613368Z"
    }
   },
   "outputs": [],
   "source": [
    "ans = \"Pakistan\"\n",
    "ans_start = context.find(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T06:21:13.159850Z",
     "start_time": "2020-05-11T06:21:10.723267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm tokens ['is', 'did', 'are', 'can', 'do', 'does', '?', 'where', 'it', 'will']\n",
      "uqa_qi_probs ['is', 'are', 'can', 'breeding', 'in', 'else', 'do', 'was', 'to', 'and']\n",
      "qi_class 0\n",
      "################\n",
      "lm tokens ['it', 'the', '?', 'its', 'found', 'this', 'that', 'located', 'from', 'a']\n",
      "uqa_qi_probs ['the', 'breeding', 'in', 'your', 'this', 'an', 'a', 'bangladesh', 'tropical', 'that']\n",
      "qi_class 1\n",
      "################\n",
      "lm tokens ['bitter', '?', 'black', 'name', 'world', 'red', 'common', 'largest', 'most', 'burma']\n",
      "uqa_qi_probs ['black', 'tropical', 'breeding', 'blue', 'white', 'african', 'red', 'negro', 'southern', 'south']\n",
      "qi_class 1\n",
      "################\n",
      "lm tokens ['##n', '##ns', '##nn', '##na', '##ne', '##m', '##nne', '##nb', '##nt', '##nia']\n",
      "uqa_qi_probs ['##n', '##ns', '##nh', '##ne', '##na', '##nst', '##s', '##no', '##ness', '##ng']\n",
      "Restarting question!!!!\n",
      "lm tokens ['is', 'did', 'are', 'can', 'do', 'does', '?', 'where', 'it', 'will']\n",
      "uqa_qi_probs ['is', 'are', 'can', 'breeding', 'in', 'else', 'do', 'was', 'to', 'and']\n",
      "qi_class 0\n",
      "################\n",
      "lm tokens ['the', 'they', 'you', 'these', 'it', 'there', '?', 'its', 'those', 'found']\n",
      "uqa_qi_probs ['you', 'the', 'breeding', 'your', 'these', 'in', 'there', 'we', 'they', 'new']\n",
      "qi_class 1\n",
      "################\n",
      "lm tokens ['bitter', '?', 'black', 'fl', 'burma', 'birds', 'two', 's', '.', 'species']\n",
      "uqa_qi_probs ['black', 'tropical', 'breeding', 'white', 'african', 'two', 'blue', 'wild', 'blacks', 'southern']\n",
      "qi_class 0\n",
      "################\n",
      "lm tokens ['bitter', 'bitterness', 'fl', 'rum', 'sore', 'sour', 'long', 'ren', 'reg', 'carp']\n",
      "uqa_qi_probs ['bitter', 'of', 'long', 'sweet', 'bitterness', 'british', 'tasting', 'carp', 'reg', 'munster']\n",
      "qi_class 1\n",
      "################\n",
      "lm tokens ['##n', '##ns', '##nn', '##nne', '##nia', '##nl', '##s', '##nni', '##nst', '##nb']\n",
      "uqa_qi_probs ['##n', '##ns', '##s', '##nn', '##nh', '##na', '##ne', '##nst', '##nin', '##nb']\n",
      "Restarting question!!!!\n",
      "lm tokens ['is', 'did', 'are', 'can', 'do', 'does', '?', 'where', 'it', 'will']\n",
      "uqa_qi_probs ['is', 'are', 'can', 'breeding', 'in', 'else', 'do', 'was', 'to', 'and']\n",
      "qi_class 0\n",
      "################\n",
      "lm tokens ['i', 'it', 'we', 'you', 'the', 'they', 'see', 'find', 'be', 'this']\n",
      "uqa_qi_probs ['i', 'you', 'we', 'breeding', 'a', 'they', 'the', 'your', 'he', 'ye']\n",
      "qi_class 1\n",
      "################\n",
      "lm tokens ['bitter', 'black', 'name', '?', 'red', 'white', 'swallow', 'is', 'grass', 'fl']\n",
      "uqa_qi_probs ['black', 'white', 'blue', 'red', 'african', 'negro', 'blacks', 'dark', 'breeding', 'tropical']\n",
      "qi_class 0\n",
      "################\n",
      "lm tokens ['bitter', 'bitterness', 'rum', 'sour', 'rag', 'long', 'reg', 'ren', 'taste', 'is']\n",
      "uqa_qi_probs ['bitter', 'sweet', 'of', 'long', 'bitterness', 'reg', 'tasting', 'taste', 'british', 'carp']\n",
      "qi_class 0\n",
      "################\n",
      "lm tokens ['##n', '##ns', '##ne', '##nn', '##m', '##na', '##nne', '##nl', '##nia', '##nst']\n",
      "uqa_qi_probs ['##n', '##ns', '##nh', '##nn', '##ne', '##s', '##na', '##nst', '##nb', '##no']\n",
      "Restarting question!!!!\n",
      "lm tokens ['is', 'did', 'are', 'can', 'do', 'does', '?', 'where', 'it', 'will']\n",
      "uqa_qi_probs ['is', 'are', 'can', 'breeding', 'in', 'else', 'do', 'was', 'to', 'and']\n",
      "qi_class 1\n",
      "################\n",
      "lm tokens ['the', 'it', 'you', 'they', 'this', 'come', 'its', 'these', 'your', 'get']\n",
      "uqa_qi_probs ['you', 'breeding', 'the', 'i', 'your', 'they', 'ye', 'we', 'he', 'bangladesh']\n",
      "qi_class 0\n",
      "################\n",
      "lm tokens ['occur', 'come', 'in', 'go', 'from', 'begin', '?', 'of', 'take', 'range']\n",
      "uqa_qi_probs ['in', 'tropical', 'and', 'out', 'is', 'breeding', 'are', 'people', 'son', 'the']\n",
      "qi_class 1\n",
      "################\n",
      "lm tokens ['?', 'in', 'from', 'to', ',', 'when', 'between', '.', 'is', 'before']\n",
      "uqa_qi_probs ['in', 'from', '?', 'breeding', 'tropical', 'and', ',', 'to', 'between', 'when']\n",
      "qi_class 0\n",
      "################\n",
      "lm tokens ['?', 'india', 'the', 'south', 'bangladesh', 'east', 'china', 'burma', 'tropical', 'north']\n",
      "uqa_qi_probs ['tropical', 'bangladesh', 'southern', 'the', 'subtropical', 'new', 'south', 'breeding', 'between', 'pan']\n",
      "qi_class 0\n",
      "################\n",
      "lm tokens ['asia', 'india', 'africa', 'america', 'china', '?', '##asia', 'asian', 'east', '.']\n",
      "uqa_qi_probs ['asia', 'asian', '##asia', 'india', 'africa', 'east', 'south', 'and', ',', 'asiatic']\n",
      "qi_class 0\n",
      "################\n",
      "lm tokens ['?', ',', 'to', 'from', '.', 'where', 'and', 'between', 'in', '[SEP]']\n",
      "uqa_qi_probs ['from', ',', 'and', '?', 'to', 'in', '(', 'between', 'breeding', 'is']\n",
      "qi_class 1\n",
      "################\n",
      "lm tokens ['[SEP]', 'a', 'come', ',', '.', '-', '?', 'o', 'dr', ')']\n",
      "uqa_qi_probs ['[SEP]', ',', \"'\", '\"', 'a', 'from', 'look', 'the', 'is', 'and']\n",
      "qi_class 1\n",
      "################\n",
      "Attempted Questions: ['where is the bitter', 'where are the black bitter', 'where can the black bitter']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'where did breeding occur in tropical asia ?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QG.generate_question(context, ans, ans_start, ['where'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
