{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary size = 30K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionGeneration():\n",
    "    def __init__(self, lm_gq, uqa_qg, regularization):\n",
    "        self.lm_qg = lm_qg\n",
    "        self.uqa_qg = uqa_qg\n",
    "        self.regularization = regularization\n",
    "        \n",
    "    def generate_token(self, context: str, ans: str, list_question_tokens: list) -> str:\n",
    "        '''\n",
    "        Input:\n",
    "            - context:\n",
    "            - ans:\n",
    "            - list_question_tokens: the history of generated question tokens.\n",
    "            The form of list_question_tokens is a list of (q_token, class), where class is 0 for lm\n",
    "            token and 1 for uqa token\n",
    "        Output:\n",
    "            - The next generated question token\n",
    "        '''\n",
    "        # contains the tokens of the generated question\n",
    "        list_question_tokens = []\n",
    "        # contains the classes of each token of the gen. question. Same len as list_question_tokens\n",
    "        # 0 = LM, 1 = UQA\n",
    "        list_token_classes = []\n",
    "        qi = None\n",
    "        \n",
    "        while not self.finished_generation(qi):\n",
    "            # Generate the probs of the ith query token using the lm and uqa models\n",
    "            lm_qi_probs = self.lm_qg.forward(context, ans, list_question_tokens)\n",
    "            uqa_qi_probs = self.uqa_qg.forward(context, ans, list_question_tokens)\n",
    "\n",
    "            # Get the final token  prob distribution using regulatization\n",
    "            qi_probs, qi_class = self.regularization(lm_qi_probs, uqa_qi_probs, \n",
    "                                                     list_question_tokens, list_token_classes)\n",
    "\n",
    "            # Get the predicted token\n",
    "            qi = torch.argmax(qi_probs, dim=0)\n",
    "            list_question_tokens.append(VOCAB_IDX2_STR[qi].item())\n",
    "            list_token_classes.append(qi_class)\n",
    "            \n",
    "            \n",
    "    def finished_generation(self, question_token):\n",
    "        return question_token == '?' or question_token =='EOS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:33:44.379956Z",
     "start_time": "2020-04-18T05:33:42.298477Z"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import numpy as np\n",
    "word_url = \"http://svnweb.freebsd.org/csrg/share/dict/words?view=co&content-type=text/plain\"\n",
    "response = urllib.request.urlopen(word_url)\n",
    "long_txt = response.read().decode()\n",
    "VOCAB_IDX2_STR = np.array(long_txt.splitlines()).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:33:44.384517Z",
     "start_time": "2020-04-18T05:33:44.381340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25487, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_IDX2_STR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:39:34.541969Z",
     "start_time": "2020-04-18T05:39:34.535600Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:39:35.041424Z",
     "start_time": "2020-04-18T05:39:35.036434Z"
    }
   },
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:40:46.146027Z",
     "start_time": "2020-04-18T05:40:46.140537Z"
    }
   },
   "outputs": [],
   "source": [
    "qi_probs = softmax(torch.empty(VOCAB_IDX2_STR.shape[0]).normal_(mean=4,std=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:40:46.497031Z",
     "start_time": "2020-04-18T05:40:46.455058Z"
    }
   },
   "outputs": [],
   "source": [
    "qi = torch.argmax(qi_probs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:41:13.161422Z",
     "start_time": "2020-04-18T05:41:13.151075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Valparaiso'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_IDX2_STR[qi].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis)",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
