{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T15:05:24.263928Z",
     "start_time": "2020-04-28T15:05:24.257399Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Freq_Regularization():\n",
    "    def predict_class(self, list_token_classes):\n",
    "        '''\n",
    "        UQA class = 0\n",
    "        LM class = 1\n",
    "        '''\n",
    "        if len(list_token_classes) == 0:\n",
    "            return np.random.choice(2, 1, p=[0.5, 0.5])[0]\n",
    "\n",
    "        prob_uqa = sum(list_token_classes)/len(list_token_classes)\n",
    "        prob_lm = 1 - prob_uqa\n",
    "        return np.random.choice(2, 1, p=[prob_uqa, prob_lm])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T15:08:47.454589Z",
     "start_time": "2020-04-28T15:08:47.416627Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from BERTQG.token_generation import load_model, generate_token\n",
    "from Regularization_module import Regularizer_Discriminator\n",
    "from itertools import groupby\n",
    "\n",
    "\n",
    "class QuestionGeneration():\n",
    "    def __init__(self, bert_model, lm_gq, uqa_qg, regularization=None):\n",
    "        self.lm_qg, _, _ = load_model(bert_model, lm_qg)\n",
    "        self.uqa_qg, self.tokenizer, self.device = load_model(bert_model, uqa_qg)\n",
    "        #self.regularization = Freq_Regularization()\n",
    "        self.regularization = Regularizer_Discriminator(regularization)\n",
    "        \n",
    "    def generate_question(self, context: str, ans: str, ans_start: str, list_question_tokens: list = []) -> str:\n",
    "        '''\n",
    "        Input:\n",
    "            - context\n",
    "            - ans\n",
    "            - ans_start\n",
    "            - list_question_tokens: the history of generated question tokens.\n",
    "            The form of list_question_tokens is a list of (q_token, class), where class is 0 for uqa\n",
    "            token and 1 for lm token\n",
    "        Output:\n",
    "            - The next generated question token\n",
    "        '''\n",
    "        # contains the tokens of the generated question\n",
    "        if len(list_question_tokens) == 0:\n",
    "          list_question_tokens = []\n",
    "        # contains the classes of each token of the gen. question. Same len as list_question_tokens\n",
    "        # 0 = UQA, 1 = LM\n",
    "        list_token_classes = []\n",
    "        qi = None\n",
    "        max_legnth = 50\n",
    "        # generation finished when [SEP] is created\n",
    "        while not self.__finished_generation(qi):\n",
    "            question_text = \" \".join(list_question_tokens)\n",
    "            \n",
    "            # Generate the toknes and probs of the ith query token using the lm and uqa models\n",
    "            lm_qi_token, lm_qi_idx, lm_qi_probs = generate_token(self.lm_qg, self.tokenizer, self.device, context, question_text, ans, ans_start)\n",
    "            uqa_qi_token, uqa_qi_idx, uqa_qi_probs = generate_token(self.uqa_qg, self.tokenizer, self.device, context, question_text, ans, ans_start)\n",
    "            \n",
    "            # Get token class to use\n",
    "            #qi_class = self.regularization.predict_class(list_token_classes)\n",
    "            qi_class= self.regularization.predict_class(context,ans,ans_start,question_text,list_question_tokens)\n",
    "            # Get the predicted token\n",
    "            if qi_class == 1: # LM\n",
    "                qi = lm_qi_token\n",
    "            else: # UQA\n",
    "                qi = uqa_qi_token\n",
    "\n",
    "            list_question_tokens.append(qi)\n",
    "            list_token_classes.append(qi_class)\n",
    "            \n",
    "#             # for testing\n",
    "#             print(list_question_tokens)\n",
    "#             print([\"UQA\" if cls == 0 else \"LM\" for cls in list_token_classes])\n",
    "#             print(\"\\n\")\n",
    "            if (len(list_question_tokens) > max_legnth):\n",
    "              break\n",
    "        list_question_tokens = self.__remove_consecutive_repeated_tokens(list_question_tokens)\n",
    "        return \" \".join(list_question_tokens[:-1]),list_token_classes # without [SEP]\n",
    "            \n",
    "    def __finished_generation(self, question_token):\n",
    "        return question_token =='[SEP]'\n",
    "    \n",
    "    def __remove_consecutive_repeated_tokens(self, list_tokens):\n",
    "        '''\n",
    "        Removes consecutive tokens.\n",
    "        Sometimes the generated question is \"when when did...\",\n",
    "        so we need to remove one when\n",
    "        '''\n",
    "        return [x[0] for x in groupby(list_tokens)]\n",
    "    #assert remove_consecutive_repeated_tokens([1,1,1,1,1,1,2,3,4,4,5,1,2]) == [1, 2, 3, 4, 5, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T15:08:52.230835Z",
     "start_time": "2020-04-28T15:08:48.699639Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/29/2020 01:19:27 - INFO - BERTQG.tokenization -   loading vocabulary file BERTQG/models/bert-base-uncased/vocab.txt\n",
      "04/29/2020 01:19:27 - INFO - BERTQG.modeling -   loading archive file BERTQG/models/bert-base-uncased\n",
      "04/29/2020 01:19:27 - INFO - BERTQG.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 3,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load BERTQG/models/lm_10k_QG/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/29/2020 01:19:29 - INFO - BERTQG.tokenization -   loading vocabulary file BERTQG/models/bert-base-uncased/vocab.txt\n",
      "04/29/2020 01:19:29 - INFO - BERTQG.modeling -   loading archive file BERTQG/models/bert-base-uncased\n",
      "04/29/2020 01:19:29 - INFO - BERTQG.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 3,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load BERTQG/models/uqa_10k_QG/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "bert_model = 'BERTQG/models/bert-base-uncased'\n",
    "lm_qg = 'BERTQG/models/lm_10k_QG/pytorch_model.bin'\n",
    "uqa_qg = 'BERTQG/models/uqa_10k_QG/pytorch_model.bin'\n",
    "regul_model = \"\"# model path here\n",
    "#QG = QuestionGeneration(bert_model, lm_qg, uqa_qg)\n",
    "QG = QuestionGeneration(bert_model, lm_qg, uqa_qg, regul_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T15:13:19.847214Z",
     "start_time": "2020-04-28T15:13:19.839996Z"
    }
   },
   "outputs": [],
   "source": [
    "context = \"The city has a proud history of theatre. Stephen Kemble of the famous Kemble family successfully managed the original Theatre Royal, Newcastle for fifteen years (1791â€“1806). He brought members of his famous acting family such as Sarah Siddons and John Kemble out of London to Newcastle. Stephen Kemble guided the theatre through many celebrated seasons. The original Theatre Royal in Newcastle was opened on 21 January 1788 and was located on Mosley Street. It was demolished to make way for Grey Street, where its replacement was built.\"\n",
    "question_text = \"when did the original theatre royal open ?\"\n",
    "ans = \"1788\"\n",
    "ans_start = context.find(ans)\n",
    "assert context[ans_start:ans_start+len(ans)] == ans\n",
    "query, history= QG.generate_question(context, ans, ans_start,[])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
