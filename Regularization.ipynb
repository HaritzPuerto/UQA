{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary size = 30K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T12:21:45.586110Z",
     "start_time": "2020-04-21T12:21:45.049065Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from BERTQG.token_generation import load_model, generate_token\n",
    "from Regularization_module import Regularizer_Discriminator\n",
    "\n",
    "class QuestionGeneration():\n",
    "    def __init__(self, bert_model, lm_gq, uqa_qg, regularization):\n",
    "        self.lm_qg, _, _ = load_model(bert_model, lm_qg)\n",
    "        self.uqa_qg, self.tokenizer, device = load_model(bert_model, uqa_qg)\n",
    "        self.regularization = Regularizer_Discriminator(regularization)\n",
    "        \n",
    "    def generate_question(self, context: str, ans: str, ans_start: int, list_question_tokens: list) -> str:\n",
    "        '''\n",
    "        Input:\n",
    "            - context\n",
    "            - ans\n",
    "            - ans_start\n",
    "            - list_question_tokens: the history of generated question tokens.\n",
    "            The form of list_question_tokens is a list of (q_token, class), where class is 0 for lm\n",
    "            token and 1 for uqa token\n",
    "        Output:\n",
    "            - The next generated question token\n",
    "        '''\n",
    "        # contains the tokens of the generated question\n",
    "        list_question_tokens = []\n",
    "        # contains the classes of each token of the gen. question. Same len as list_question_tokens\n",
    "        # 0 = LM, 1 = UQA\n",
    "        list_token_classes = []\n",
    "        qi = None\n",
    "        \n",
    "        # generation finished when [SEP] is created\n",
    "        while not self.finished_generation(qi):\n",
    "            question_text = \" \".jon(list_question_tokens)\n",
    "            \n",
    "            # Generate the toknes and probs of the ith query token using the lm and uqa models\n",
    "            lm_qi_token, lm_qi_idx, lm_qi_probs = generate_token(lm_qg, tokenizer, device, context, question_text, ans, ans_start)\n",
    "            uqa_qi_token, uqa_qi_idx, uqa_qi_probs = generate_token(uqa_qg, tokenizer, device, context, question_text, ans, ans_start)\n",
    "            \n",
    "            # Get the final token  prob distribution using regulatization\n",
    "            jsonobj = self.__convert2squad(context, ans, ans_start, question_text)\n",
    "            qi_probs = self.regularization.predict_prob(jsonobj) #qi_probs: [batchsize,2] ndarray\n",
    "            qi_class = np.argmin(qi_probs, axis=1) #qi_class: [batchsize,] ndarray 0:should generate UQA, 1:should generate LM\n",
    "            # Get the predicted token\n",
    "            if qi_class == 1: # LM\n",
    "                qi = lm_qi_token\n",
    "            else: # UQA\n",
    "                qi = uqa_qi_token\n",
    "                \n",
    "            list_question_tokens.append(qi)\n",
    "            list_token_classes.append(qi_class)\n",
    "\n",
    "        return \" \".join(list_question_tokens)\n",
    "            \n",
    "    def finished_generation(self, question_token):\n",
    "        return question_token =='[SEP]'\n",
    "    \n",
    "    def __convert2squad(self, context: str, answer: str, ans_start: int,  question: str) -> dict:\n",
    "        '''\n",
    "        Create a SQuAD instance\n",
    "        Inputs:\n",
    "            - context: paragrah\n",
    "            - answer\n",
    "            - ans_start\n",
    "            - question: might not be the full question (we are generating questions token by token)\n",
    "        Returns:\n",
    "            - squad instance\n",
    "        '''\n",
    "        squad = {'data': [], 'version': '1.0'}\n",
    "        squad['data'].append({'paragraphs': [{'title': 'title', \n",
    "                                              'context': context,\n",
    "                                              'qas': [{'answers': [{'answer_start': ans_start, 'text': answer}],\n",
    "                                                       'question': question,\n",
    "                                                       'id': 0}]}]})\n",
    "        return squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T12:22:00.704850Z",
     "start_time": "2020-04-21T12:21:49.009922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9e639c907a48238fc100d2368cf4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=361.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd20fff5666b4733b83e493701da3866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_model = 'BERTQG/models/bert-base-uncased'\n",
    "lm_qg = 'BERTQG/models/lm_10k_QG/pytorch_model.bin'\n",
    "uqa_qg = 'BERTQG/models/uqa_50k_QG/pytorch_model_60000.bin'\n",
    "regul_model = \"./discri_model.bin\"# model path here\n",
    "QG = QuestionGeneration(bert_model, lm_qg, uqa_qg, regul_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T08:29:17.876864Z",
     "start_time": "2020-04-21T08:29:16.182043Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../datasets/squad_NER_ans_full_wo_questions_evenly_distributed.json', 'r') as f:\n",
    "    squad = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:33:44.379956Z",
     "start_time": "2020-04-18T05:33:42.298477Z"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import numpy as np\n",
    "word_url = \"http://svnweb.freebsd.org/csrg/share/dict/words?view=co&content-type=text/plain\"\n",
    "response = urllib.request.urlopen(word_url)\n",
    "long_txt = response.read().decode()\n",
    "VOCAB_IDX2_STR = np.array(long_txt.splitlines()).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:33:44.384517Z",
     "start_time": "2020-04-18T05:33:44.381340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25487, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_IDX2_STR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:39:34.541969Z",
     "start_time": "2020-04-18T05:39:34.535600Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:39:35.041424Z",
     "start_time": "2020-04-18T05:39:35.036434Z"
    }
   },
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:40:46.146027Z",
     "start_time": "2020-04-18T05:40:46.140537Z"
    }
   },
   "outputs": [],
   "source": [
    "qi_probs = softmax(torch.empty(VOCAB_IDX2_STR.shape[0]).normal_(mean=4,std=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:40:46.497031Z",
     "start_time": "2020-04-18T05:40:46.455058Z"
    }
   },
   "outputs": [],
   "source": [
    "qi = torch.argmax(qi_probs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:41:13.161422Z",
     "start_time": "2020-04-18T05:41:13.151075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Valparaiso'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_IDX2_STR[qi].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (transformernew)",
   "language": "python",
   "name": "transformernew"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
