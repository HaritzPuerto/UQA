{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T14:49:52.492029Z",
     "start_time": "2020-05-11T14:49:52.487859Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Freq_Regularization():\n",
    "    def predict_class(self, list_token_classes):\n",
    "        '''\n",
    "        UQA class = 0\n",
    "        LM class = 1\n",
    "        '''\n",
    "        if len(list_token_classes) == 0:\n",
    "            return np.random.choice(2, 1, p=[0.5, 0.5])[0]\n",
    "\n",
    "        prob_uqa = sum(list_token_classes)/len(list_token_classes)\n",
    "        prob_lm = 1 - prob_uqa\n",
    "        return np.random.choice(2, 1, p=[prob_uqa, prob_lm])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T16:05:13.077694Z",
     "start_time": "2020-05-11T16:05:13.031591Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from BERTQG.token_generation import load_model, generate_token\n",
    "from Regularization_module import Regularizer_Discriminator\n",
    "from itertools import groupby\n",
    "\n",
    "\n",
    "class QuestionGeneration():\n",
    "    def __init__(self, bert_model, lm_gq, uqa_qg, regularization=None):\n",
    "        self.lm_qg, _, _ = load_model(bert_model, lm_qg)\n",
    "        self.uqa_qg, self.tokenizer, self.device = load_model(bert_model, uqa_qg)\n",
    "        #self.regularization = Freq_Regularization()\n",
    "        self.regularization = Regularizer_Discriminator(regularization)\n",
    "        \n",
    "    def generate_question(self, context: str, ans: str, ans_start: str, list_question_tokens: list = []) -> str:\n",
    "        '''\n",
    "        Input:\n",
    "            - context\n",
    "            - ans\n",
    "            - ans_start\n",
    "            - list_question_tokens: the history of generated question tokens. This is given for testing.\n",
    "            The form of list_question_tokens is a list of (q_token, class), where class is 0 for uqa\n",
    "            token and 1 for lm token\n",
    "        Output:\n",
    "            - The next generated question token\n",
    "        '''\n",
    "        # contains the tokens of the generated question\n",
    "        if len(list_question_tokens) == 0:\n",
    "            list_question_tokens = []\n",
    "            list_token_classes = [] # 0 = UQA, 1 = LM\n",
    "            list_qi_idx = [] \n",
    "            list_qi_probs = []\n",
    "        else: # for testing\n",
    "            len_initial_tokens = len(list_question_tokens)\n",
    "            list_token_classes = [-1] * len_initial_tokens # 0 = UQA, 1 = LM\n",
    "            list_qi_idx = [-1] * len_initial_tokens\n",
    "            list_qi_probs = [-1] * len_initial_tokens\n",
    "\n",
    "        # contains the classes of each token of the gen. question. Same len as list_question_tokens\n",
    "        qi = qi_idx = qi_probs = None\n",
    "        max_legnth = 50\n",
    "        # generation finished when [SEP] is created\n",
    "        while not self.__finished_generation(qi):\n",
    "            question_text = \" \".join(list_question_tokens)\n",
    "            \n",
    "            # Generate the toknes and probs of the ith query token using the lm and uqa models\n",
    "            lm_qi_token, lm_qi_idx, lm_qi_probs = generate_token(self.lm_qg, self.tokenizer, self.device, context, question_text, ans, ans_start)\n",
    "            uqa_qi_token, uqa_qi_idx, uqa_qi_probs = generate_token(self.uqa_qg, self.tokenizer, self.device, context, question_text, ans, ans_start)\n",
    "            \n",
    "            # Get token class to use\n",
    "            #qi_class = self.regularization.predict_class(list_token_classes)\n",
    "            qi_class= self.regularization.predict_class(context,ans,ans_start,question_text,list_question_tokens)\n",
    "            # Get the predicted token\n",
    "            if qi_class == 1: # LM\n",
    "                qi = lm_qi_token\n",
    "                qi_idx = lm_qi_idx\n",
    "                qi_probs = lm_qi_probs\n",
    "            else: # UQA\n",
    "                qi = uqa_qi_token\n",
    "                qi_idx = uqa_qi_idx\n",
    "                qi_probs = uqa_qi_probs\n",
    "            \n",
    "            list_question_tokens.append(qi)\n",
    "            list_token_classes.append(qi_class)\n",
    "            list_qi_idx.append(qi_idx)\n",
    "            list_qi_probs.append(qi_probs)\n",
    "            \n",
    "#             # for testing\n",
    "#             print(list_question_tokens)\n",
    "#             print([\"UQA\" if cls == 0 else \"LM\" for cls in list_token_classes])\n",
    "#             print(\"\\n\")\n",
    "            if (len(list_question_tokens) > max_legnth):\n",
    "              break\n",
    "        \n",
    "        # indices to keep\n",
    "        list_idx = self.__remove_consecutive_repeated_tokens(list_question_tokens)\n",
    "\n",
    "        # without [SEP] -> [:-1]\n",
    "        list_question_tokens = [list_question_tokens[idx] for idx in list_idx][:-1]\n",
    "        list_token_classes = [list_token_classes[idx] for idx in list_idx][:-1]\n",
    "        list_qi_idx = [list_qi_idx[idx] for idx in list_idx][:-1]\n",
    "        list_qi_probs = [list_qi_probs[idx] for idx in list_idx][:-1]\n",
    "        \n",
    "        assert len(list_question_tokens) == len(list_token_classes) == len(list_qi_idx) == len(list_qi_probs)\n",
    "\n",
    "        return \" \".join(list_question_tokens), list_token_classes, list_qi_idx, list_qi_probs\n",
    "            \n",
    "    def __finished_generation(self, question_token):\n",
    "        return question_token =='[SEP]'\n",
    "    \n",
    "#     def __remove_consecutive_repeated_tokens(self, list_tokens):\n",
    "#         '''\n",
    "#         Removes consecutive tokens.\n",
    "#         Sometimes the generated question is \"when when did...\",\n",
    "#         so we need to remove one when\n",
    "#         '''\n",
    "#         return [x[0] for x in groupby(list_tokens)]\n",
    "    #assert remove_consecutive_repeated_tokens([1,1,1,1,1,1,2,3,4,4,5,1,2]) == [1, 2, 3, 4, 5, 1, 2]\n",
    "  \n",
    "    def __remove_consecutive_repeated_tokens(self, list_tokens):\n",
    "        '''\n",
    "        Removes consecutive tokens.\n",
    "        Sometimes the generated question is \"when when did...\",\n",
    "        so we need to remove one when\n",
    "        Output:\n",
    "            - list of index that is not consecutive repeated\n",
    "            - list of index to keep\n",
    "        '''\n",
    "        indices = range(len(list_tokens))\n",
    "        return [list(group)[0][1] for key, group in groupby(zip(list_tokens, indices), lambda x: x[0])]\n",
    "#     assert __remove_consecutive_repeated_tokens([1,1,1,1,1,1,2,3,4,4,5,1,2]) == [0, 6, 7, 8, 10, 11, 12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T16:05:21.746888Z",
     "start_time": "2020-05-11T16:05:13.531126Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/11/2020 16:05:13 - INFO - BERTQG.tokenization -   loading vocabulary file BERTQG/models/bert-base-uncased/vocab.txt\n",
      "05/11/2020 16:05:13 - INFO - BERTQG.modeling -   loading archive file BERTQG/models/bert-base-uncased\n",
      "05/11/2020 16:05:13 - INFO - BERTQG.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 3,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load BERTQG/models/lm_10k_QG/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/11/2020 16:05:15 - INFO - BERTQG.tokenization -   loading vocabulary file BERTQG/models/bert-base-uncased/vocab.txt\n",
      "05/11/2020 16:05:15 - INFO - BERTQG.modeling -   loading archive file BERTQG/models/bert-base-uncased\n",
      "05/11/2020 16:05:15 - INFO - BERTQG.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 3,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load BERTQG/models/uqa_10k_QG/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/11/2020 16:05:18 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "05/11/2020 16:05:18 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "05/11/2020 16:05:18 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "05/11/2020 16:05:19 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "05/11/2020 16:05:19 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_model = 'BERTQG/models/bert-base-uncased'\n",
    "lm_qg = 'BERTQG/models/lm_10k_QG/pytorch_model.bin'\n",
    "uqa_qg = 'BERTQG/models/uqa_10k_QG/pytorch_model.bin'\n",
    "regul_model = 'models/discri_model_partial_perturb.bin' # model path here\n",
    "#QG = QuestionGeneration(bert_model, lm_qg, uqa_qg)\n",
    "QG = QuestionGeneration(bert_model, lm_qg, uqa_qg, regul_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T16:05:21.751350Z",
     "start_time": "2020-05-11T16:05:21.748508Z"
    }
   },
   "outputs": [],
   "source": [
    "context = \"The city has a proud history of theatre. Stephen Kemble of the famous Kemble family successfully managed the original Theatre Royal, Newcastle for fifteen years (1791–1806). He brought members of his famous acting family such as Sarah Siddons and John Kemble out of London to Newcastle. Stephen Kemble guided the theatre through many celebrated seasons. The original Theatre Royal in Newcastle was opened on 21 January 1788 and was located on Mosley Street. It was demolished to make way for Grey Street, where its replacement was built.\"\n",
    "# question_text = \"when did the original theatre royal open ?\"\n",
    "ans = \"1788\"\n",
    "ans_start = context.find(ans)\n",
    "assert context[ans_start:ans_start+len(ans)] == ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T16:05:25.036444Z",
     "start_time": "2020-05-11T16:05:21.753173Z"
    }
   },
   "outputs": [],
   "source": [
    "question, history, indices, probs = QG.generate_question(context, ans, ans_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T16:05:25.041711Z",
     "start_time": "2020-05-11T16:05:25.038555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'when did the original theatre royal open on 21 january ?'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T16:05:25.112073Z",
     "start_time": "2020-05-11T16:05:25.043097Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T16:05:25.187083Z",
     "start_time": "2020-05-11T16:05:25.117847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2043, 2106, 1996, 2434, 3004, 2548, 2330, 2006, 2538, 2254, 1029]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T16:05:25.261998Z",
     "start_time": "2020-05-11T16:05:25.192741Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00454629585146904"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[-1][102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T16:05:27.313778Z",
     "start_time": "2020-05-11T16:05:27.305731Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_top_k(probs, k):\n",
    "    top_k_indices = [np.argsort(-np.array(probs[token_idx]))[:k].tolist() for token_idx in range(len(probs))]\n",
    "    top_k_probs = [sorted(probs[token_idx], reverse=True)[:k] for token_idx in range(len(probs))]\n",
    "    return top_k_indices, top_k_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T16:05:29.206113Z",
     "start_time": "2020-05-11T16:05:29.122973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1029, 1012, 13739, 1010, 2960, 15622, 13393, 1024, 2997, 1011]\n",
      "[0.6433014273643494, 0.10247533023357391, 0.03994838520884514, 0.027439186349511147, 0.025517335161566734, 0.02007225900888443, 0.007837814278900623, 0.007773424033075571, 0.006851928308606148, 0.005206955596804619]\n"
     ]
    }
   ],
   "source": [
    "top_k_indices, top_k_probs = get_top_k(probs, 10)\n",
    "print(top_k_indices[-1])\n",
    "print(top_k_probs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T15:59:20.729665Z",
     "start_time": "2020-05-11T15:58:02.095178Z"
    }
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "k = 10\n",
    "for i in range(1000):\n",
    "    top_k_indices, top_k_probs = get_top_k(probs, k)\n",
    "    assert len(question.split()) == len(top_k_indices) == len(top_k_probs)\n",
    "    paragraph = {\n",
    "                'qid': i,\n",
    "                'context': context,\n",
    "                'question': question,\n",
    "                'top_k_indices': top_k_indices,\n",
    "                'top_k_probs': top_k_probs,\n",
    "                'answers': ans,\n",
    "                'answer_start': ans_start\n",
    "                }\n",
    "    output.append(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T15:12:49.505706Z",
     "start_time": "2020-05-11T15:12:49.497506Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T15:13:10.255688Z",
     "start_time": "2020-05-11T15:13:09.973995Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('soft_target_test.json', 'w') as f:\n",
    "    json.dump(output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
