{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary size = 30K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T15:05:24.263928Z",
     "start_time": "2020-04-28T15:05:24.257399Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Freq_Regularization():\n",
    "    def predict_class(self, list_token_classes):\n",
    "        '''\n",
    "        UQA class = 0\n",
    "        LM class = 1\n",
    "        '''\n",
    "        if len(list_token_classes) == 0:\n",
    "            return np.random.choice(2, 1, p=[0.5, 0.5])[0]\n",
    "\n",
    "        prob_uqa = sum(list_token_classes)/len(list_token_classes)\n",
    "        prob_lm = 1 - prob_uqa\n",
    "        return np.random.choice(2, 1, p=[prob_uqa, prob_lm])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T15:08:47.454589Z",
     "start_time": "2020-04-28T15:08:47.416627Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from BERTQG.token_generation import load_model, generate_token\n",
    "#from Regularization_module import Regularizer_Discriminator\n",
    "\n",
    "class QuestionGeneration():\n",
    "    def __init__(self, bert_model, lm_gq, uqa_qg, regularization=None):\n",
    "        self.lm_qg, _, _ = load_model(bert_model, lm_qg)\n",
    "        self.uqa_qg, self.tokenizer, self.device = load_model(bert_model, uqa_qg)\n",
    "        self.regularization = Freq_Regularization()\n",
    "        \n",
    "    def generate_question(self, context: str, ans: str, ans_start: str, list_question_tokens: list = []) -> str:\n",
    "        '''\n",
    "        Input:\n",
    "            - context\n",
    "            - ans\n",
    "            - ans_start\n",
    "            - list_question_tokens: the history of generated question tokens.\n",
    "            The form of list_question_tokens is a list of (q_token, class), where class is 0 for uqa\n",
    "            token and 1 for lm token\n",
    "        Output:\n",
    "            - The next generated question token\n",
    "        '''\n",
    "        # contains the tokens of the generated question\n",
    "        list_question_tokens = []\n",
    "        # contains the classes of each token of the gen. question. Same len as list_question_tokens\n",
    "        # 0 = UQA, 1 = LM\n",
    "        list_token_classes = []\n",
    "        qi = None\n",
    "        \n",
    "        # generation finished when [SEP] is created\n",
    "        while not self.finished_generation(qi):\n",
    "            question_text = \" \".join(list_question_tokens)\n",
    "            \n",
    "            # Generate the toknes and probs of the ith query token using the lm and uqa models\n",
    "            lm_qi_token, lm_qi_idx, lm_qi_probs = generate_token(self.lm_qg, self.tokenizer, self.device, context, question_text, ans, ans_start)\n",
    "            uqa_qi_token, uqa_qi_idx, uqa_qi_probs = generate_token(self.uqa_qg, self.tokenizer, self.device, context, question_text, ans, ans_start)\n",
    "            \n",
    "            # Get token class to use\n",
    "            qi_class = self.regularization.predict_class(list_token_classes)\n",
    "            # Get the predicted token\n",
    "            if qi_class == 1: # LM\n",
    "                qi = lm_qi_token\n",
    "            else: # UQA\n",
    "                qi = uqa_qi_token\n",
    "                \n",
    "            list_question_tokens.append(qi)\n",
    "            list_token_classes.append(qi_class)\n",
    "            \n",
    "            # for testing\n",
    "            print(list_question_tokens)\n",
    "            print([\"UQA\" if cls == 0 else \"LM\" for cls in list_token_classes])\n",
    "            print(\"\\n\")\n",
    "\n",
    "        return \" \".join(list_question_tokens[:-1]) # without [SEP]\n",
    "            \n",
    "    def finished_generation(self, question_token):\n",
    "        return question_token =='[SEP]'\n",
    "    \n",
    "    def __convert2squad(self, context: str, answer: str, ans_start: int,  question: str) -> dict:\n",
    "        '''\n",
    "        Create a SQuAD instance\n",
    "        Inputs:\n",
    "            - context: paragrah\n",
    "            - answer\n",
    "            - ans_start\n",
    "            - question: might not be the full question (we are generating questions token by token)\n",
    "        Returns:\n",
    "            - squad instance\n",
    "        '''\n",
    "        squad = {'data': [], 'version': '1.0'}\n",
    "        squad['data'].append({'paragraphs': [{'title': 'title', \n",
    "                                              'context': context,\n",
    "                                              'qas': [{'answers': [{'answer_start': ans_start, 'text': answer}],\n",
    "                                                       'question': question,\n",
    "                                                       'id': 0}]}]})\n",
    "        return squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T15:08:52.230835Z",
     "start_time": "2020-04-28T15:08:48.699639Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/29/2020 00:57:33 - INFO - BERTQG.tokenization -   loading vocabulary file BERTQG/models/bert-base-uncased/vocab.txt\n",
      "04/29/2020 00:57:33 - INFO - BERTQG.modeling -   loading archive file BERTQG/models/bert-base-uncased\n",
      "04/29/2020 00:57:33 - INFO - BERTQG.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 3,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load BERTQG/models/lm_10k_QG/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/29/2020 00:57:35 - INFO - BERTQG.tokenization -   loading vocabulary file BERTQG/models/bert-base-uncased/vocab.txt\n",
      "04/29/2020 00:57:35 - INFO - BERTQG.modeling -   loading archive file BERTQG/models/bert-base-uncased\n",
      "04/29/2020 00:57:35 - INFO - BERTQG.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 3,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load BERTQG/models/uqa_10k_QG/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "bert_model = 'BERTQG/models/bert-base-uncased'\n",
    "lm_qg = 'BERTQG/models/lm_10k_QG/pytorch_model.bin'\n",
    "uqa_qg = 'BERTQG/models/uqa_10k_QG/pytorch_model.bin'\n",
    "#regul_model = \"./discri_model.bin\"# model path here\n",
    "QG = QuestionGeneration(bert_model, lm_qg, uqa_qg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T14:10:38.047781Z",
     "start_time": "2020-04-28T14:10:38.040936Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/workspace/ml-workspace/emnlp_qg/datasets/squad_train_evenly_distributed.json', 'r') as f:\n",
    "    squad = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = squad['data'][0]['paragraphs'][0]['context']\n",
    "# ans = squad['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['text']\n",
    "# ans_start = squad['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['answer_start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T15:13:19.847214Z",
     "start_time": "2020-04-28T15:13:19.839996Z"
    }
   },
   "outputs": [],
   "source": [
    "context = \"The city has a proud history of theatre. Stephen Kemble of the famous Kemble family successfully managed the original Theatre Royal, Newcastle for fifteen years (1791â€“1806). He brought members of his famous acting family such as Sarah Siddons and John Kemble out of London to Newcastle. Stephen Kemble guided the theatre through many celebrated seasons. The original Theatre Royal in Newcastle was opened on 21 January 1788 and was located on Mosley Street. It was demolished to make way for Grey Street, where its replacement was built.\"\n",
    "question_text = \"when did the original theatre royal open ?\"\n",
    "ans = \"1788\"\n",
    "ans_start = context.find(ans)\n",
    "assert context[ans_start:ans_start+len(ans)] == ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T15:14:51.167492Z",
     "start_time": "2020-04-28T15:14:50.525903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when']\n",
      "['UQA']\n",
      "\n",
      "\n",
      "['when', 'did']\n",
      "['UQA', 'LM']\n",
      "\n",
      "\n",
      "['when', 'did', 'the']\n",
      "['UQA', 'LM', 'LM']\n",
      "\n",
      "\n",
      "['when', 'did', 'the', 'original']\n",
      "['UQA', 'LM', 'LM', 'LM']\n",
      "\n",
      "\n",
      "['when', 'did', 'the', 'original', 'theatre']\n",
      "['UQA', 'LM', 'LM', 'LM', 'UQA']\n",
      "\n",
      "\n",
      "['when', 'did', 'the', 'original', 'theatre', 'royal']\n",
      "['UQA', 'LM', 'LM', 'LM', 'UQA', 'UQA']\n",
      "\n",
      "\n",
      "['when', 'did', 'the', 'original', 'theatre', 'royal', 'open']\n",
      "['UQA', 'LM', 'LM', 'LM', 'UQA', 'UQA', 'LM']\n",
      "\n",
      "\n",
      "['when', 'did', 'the', 'original', 'theatre', 'royal', 'open', 'on']\n",
      "['UQA', 'LM', 'LM', 'LM', 'UQA', 'UQA', 'LM', 'UQA']\n",
      "\n",
      "\n",
      "['when', 'did', 'the', 'original', 'theatre', 'royal', 'open', 'on', '21']\n",
      "['UQA', 'LM', 'LM', 'LM', 'UQA', 'UQA', 'LM', 'UQA', 'LM']\n",
      "\n",
      "\n",
      "['when', 'did', 'the', 'original', 'theatre', 'royal', 'open', 'on', '21', 'january']\n",
      "['UQA', 'LM', 'LM', 'LM', 'UQA', 'UQA', 'LM', 'UQA', 'LM', 'LM']\n",
      "\n",
      "\n",
      "['when', 'did', 'the', 'original', 'theatre', 'royal', 'open', 'on', '21', 'january', '?']\n",
      "['UQA', 'LM', 'LM', 'LM', 'UQA', 'UQA', 'LM', 'UQA', 'LM', 'LM', 'UQA']\n",
      "\n",
      "\n",
      "['when', 'did', 'the', 'original', 'theatre', 'royal', 'open', 'on', '21', 'january', '?', '[SEP]']\n",
      "['UQA', 'LM', 'LM', 'LM', 'UQA', 'UQA', 'LM', 'UQA', 'LM', 'LM', 'UQA', 'UQA']\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'when did the original theatre royal open on 21 january ?'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QG.generate_question(context, ans, ans_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:33:44.379956Z",
     "start_time": "2020-04-18T05:33:42.298477Z"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import numpy as np\n",
    "word_url = \"http://svnweb.freebsd.org/csrg/share/dict/words?view=co&content-type=text/plain\"\n",
    "response = urllib.request.urlopen(word_url)\n",
    "long_txt = response.read().decode()\n",
    "VOCAB_IDX2_STR = np.array(long_txt.splitlines()).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:33:44.384517Z",
     "start_time": "2020-04-18T05:33:44.381340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25487, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_IDX2_STR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:39:34.541969Z",
     "start_time": "2020-04-18T05:39:34.535600Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:39:35.041424Z",
     "start_time": "2020-04-18T05:39:35.036434Z"
    }
   },
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:40:46.146027Z",
     "start_time": "2020-04-18T05:40:46.140537Z"
    }
   },
   "outputs": [],
   "source": [
    "qi_probs = softmax(torch.empty(VOCAB_IDX2_STR.shape[0]).normal_(mean=4,std=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:40:46.497031Z",
     "start_time": "2020-04-18T05:40:46.455058Z"
    }
   },
   "outputs": [],
   "source": [
    "qi = torch.argmax(qi_probs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:41:13.161422Z",
     "start_time": "2020-04-18T05:41:13.151075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Valparaiso'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_IDX2_STR[qi].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
