{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T11:58:59.266780Z",
     "start_time": "2020-05-12T11:58:59.206972Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Freq_Regularization():\n",
    "    def predict_class(self, list_token_classes):\n",
    "        '''\n",
    "        UQA class = 0\n",
    "        LM class = 1\n",
    "        '''\n",
    "        if len(list_token_classes) == 0:\n",
    "            return np.random.choice(2, 1, p=[0.5, 0.5])[0]\n",
    "\n",
    "        prob_uqa = sum(list_token_classes)/len(list_token_classes)\n",
    "        prob_lm = 1 - prob_uqa\n",
    "        return np.random.choice(2, 1, p=[prob_uqa, prob_lm])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T13:43:24.053493Z",
     "start_time": "2020-05-12T13:43:24.032009Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-45-153cba79b36d>, line 116)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-45-153cba79b36d>\"\u001b[0;36m, line \u001b[0;32m116\u001b[0m\n\u001b[0;31m    (num_generated_tokens == 3 and predicted_token == \"?\" and prev_token in set(stopwords.words('english'))):\u001b[0m\n\u001b[0m                                                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from BERTQG.token_generation import load_model, generate_token\n",
    "#from Regularization_module import Regularizer_Discriminator\n",
    "from itertools import groupby\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# you might need nltk.download('stopwords')\n",
    "\n",
    "class QuestionGeneration():\n",
    "    def __init__(self, bert_model, lm_gq, uqa_qg, regularization=None):\n",
    "        self.lm_qg, _, _ = load_model(bert_model, lm_qg)\n",
    "        self.uqa_qg, self.tokenizer, self.device = load_model(bert_model, uqa_qg)\n",
    "        self.regularization = Freq_Regularization()\n",
    "        \n",
    "        # get list of ids of ##n, ##na, ...\n",
    "        list_bert_token_vocab  = self.tokenizer.convert_ids_to_tokens(list(range(len(self.tokenizer.vocab.keys()))))\n",
    "        self.list_subtoken_ids = [token_id for token_id, token in \n",
    "                                  enumerate(list_bert_token_vocab) if \"##\" in token]\n",
    "\n",
    "        self.list_wh = ['what', 'which', 'where', 'when', 'who', 'why', 'how', 'whom', 'whose']\n",
    "        self.list_wh_ids = self.tokenizer.convert_tokens_to_ids(self.list_wh)\n",
    "\n",
    "        self.list_pronouns = ['you', 'it', 'they', 'he', 'she', \"i\", 'we']\n",
    "        self.list_pronouns_ids = self.tokenizer.convert_tokens_to_ids(self.list_pronouns)\n",
    "        \n",
    "        \n",
    "    def generate_question(self, context: str, ans: str, ans_start: str, list_question_tokens: list = []) -> str:\n",
    "        '''\n",
    "        Input:\n",
    "            - context\n",
    "            - ans\n",
    "            - ans_start\n",
    "            - list_question_tokens: the history of generated question tokens.\n",
    "            The form of list_question_tokens is a list of (q_token, class), where class is 0 for uqa\n",
    "            token and 1 for lm token\n",
    "        Output:\n",
    "            - The next generated question token\n",
    "        '''\n",
    "        # contains the classes of each token of the gen. question. Same len as list_question_tokens\n",
    "        # 0 = UQA, 1 = LM\n",
    "        list_token_classes = []\n",
    "        qi = None\n",
    "        \n",
    "        # generation finished when [SEP] is created\n",
    "        while not self.__finished_generation(qi):\n",
    "            question_text = \" \".join(list_question_tokens)\n",
    "            \n",
    "            ## Generate tokens ##\n",
    "            lm_qi_token, lm_qi_idx, lm_qi_probs = generate_token(self.lm_qg, self.tokenizer, self.device, context, question_text, ans, ans_start)\n",
    "            uqa_qi_token, uqa_qi_idx, uqa_qi_probs = generate_token(self.uqa_qg, self.tokenizer, self.device, context, question_text, ans, ans_start)\n",
    "            \n",
    "            print(\"lm tokens\", self.__get_topk_tokens(lm_qi_probs))\n",
    "            print(\"uqa_qi_probs\", self.__get_topk_tokens(uqa_qi_probs))\n",
    "            \n",
    "            ## Regularization ##\n",
    "            qi_class = self.regularization.predict_class(list_token_classes)\n",
    "            \n",
    "            \n",
    "            ## Postprocessing ##\n",
    "            prev_token = list_question_tokens[-1]\n",
    "            num_generated_tokens = len(list_question_tokens)\n",
    "            if qi_class == 1:\n",
    "                lm_qi_probs = self.__postprocessing(lm_qi_probs, num_generated_tokens, prev_token)\n",
    "                print(\"updated lm tokens\", self.__get_topk_tokens(lm_qi_probs))\n",
    "                qi = self.__get_topk_tokens(lm_qi_probs, 1)[0]  \n",
    "            else:\n",
    "                uqa_qi_probs = self.__postprocessing(uqa_qi_probs, num_generated_tokens, prev_token)\n",
    "                print(\"updated uqa_qi_probs\", self.__get_topk_tokens(uqa_qi_probs))\n",
    "                qi = self.__get_topk_tokens(uqa_qi_probs, 1)[0]  \n",
    "                \n",
    "            print(\"qi_class\", qi_class)\n",
    "            print(\"################\")\n",
    "            list_question_tokens.append(qi)\n",
    "            list_token_classes.append(qi_class)\n",
    "            \n",
    "#             # for testing\n",
    "#             print(list_question_tokens)\n",
    "#             print([\"UQA\" if cls == 0 else \"LM\" for cls in list_token_classes])\n",
    "#             print(\"\\n\")\n",
    "\n",
    "\n",
    "        list_question_tokens = self.__remove_consecutive_repeated_tokens(list_question_tokens)\n",
    "        return \" \".join(list_question_tokens[:-1]) # without [SEP]\n",
    "            \n",
    "    def __finished_generation(self, question_token):\n",
    "        return question_token =='[SEP]'\n",
    "    \n",
    "    def __prob_distrib2tokens(self, probs):\n",
    "        return self.tokenizer.convert_ids_to_tokens(probs)\n",
    "    \n",
    "    def __postprocessing(self, tensor_probs: torch.tensor, num_generated_tokens: int, prev_token: str):\n",
    "        '''\n",
    "        Postprocessing to remove the noise of the output distribution\n",
    "        1) Remove subwords like ##n. In most of the cases it is benefitial because \n",
    "        the QG generates ##n with a very high prob. But in same cases QG also needs to generate subwords\n",
    "        like Fuji ##mori\n",
    "        2) Remove wh word. Only needed for the first token of the question\n",
    "        3) Remove pronouns. SQuAD-like questions do not need \"you\" or \"I\"\n",
    "        4) Avoid generating short questions like \"what is?\" or what is the?\n",
    "        '''\n",
    "        \n",
    "        # 1) remove ##n\n",
    "        tensor_probs = self.__remove_subwords(tensor_probs)\n",
    "        \n",
    "        # 2) Remove wh words\n",
    "        tensor_probs = self.__remove_wh_words(tensor_probs)\n",
    "        \n",
    "        # 3) Remove pronouns. I, you, he, she, we, they are not used in SQuAD-like questions (factual)\n",
    "        tensor_probs = self.__remove_pronouns(tensor_probs)\n",
    "        \n",
    "          \n",
    "        # 4) avoid generating short questions (at least we need 3 tokens)\n",
    "        predicted_ids = reversed(np.argsort(tensor_probs)[-1:]).tolist()\n",
    "        predicted_token = self.__get_topk_tokens(tensor_probs, 1)[0]     \n",
    "        if ( (num_generated_tokens == 2 and predicted_token == \"?\") or \n",
    "             (num_generated_tokens == 3 and predicted_token == \"?\" and prev_token in set(stopwords.words('english'))):\n",
    "            tensor_probs = self.__ignore_probs(tensor_probs, predicted_ids)\n",
    "        return tensor_probs \n",
    "        \n",
    "    def __remove_subwords(self, tensor_probs):\n",
    "        return self.__ignore_probs(tensor_probs, self.list_subtoken_ids)\n",
    "            \n",
    "    def __remove_wh_words(self, tensor_probs):\n",
    "        return self.__ignore_probs(tensor_probs, self.list_wh_ids)\n",
    "\n",
    "    def __remove_pronouns(self, tensor_probs):\n",
    "        return self.__ignore_probs(tensor_probs, self.list_pronouns_ids)\n",
    "            \n",
    "    def __get_topk_tokens(self, probs, topk=10):\n",
    "        '''\n",
    "        Input:\n",
    "            - probs: probability distribution over vocab (output distribution)\n",
    "            - topk: k to return\n",
    "        Output:\n",
    "            - list of tokens (top 1 to k)\n",
    "        '''\n",
    "        return self.tokenizer.convert_ids_to_tokens(reversed(np.argsort(probs.tolist())[-topk:]))\n",
    "\n",
    "    def __ignore_probs(self, probs, idx_list):\n",
    "        '''\n",
    "        Input:\n",
    "            - probs: probability distribution over vocab (output distribution)\n",
    "            - idx_list: index list to ignore (to force it as -inf)\n",
    "        Output:\n",
    "            - new probs where probabilities of some tokens are ignored\n",
    "        '''\n",
    "        for idx in idx_list:\n",
    "            probs[idx] = -float('inf')\n",
    "        return probs\n",
    "        \n",
    "    def __remove_consecutive_repeated_tokens(self, list_tokens):\n",
    "        '''\n",
    "        Removes consecutive tokens.\n",
    "        Sometimes the generated question is \"when when did...\",\n",
    "        so we need to remove one when\n",
    "        '''\n",
    "        return [x[0] for x in groupby(list_tokens)]\n",
    "    #assert remove_consecutive_repeated_tokens([1,1,1,1,1,1,2,3,4,4,5,1,2]) == [1, 2, 3, 4, 5, 1, 2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T13:43:29.145914Z",
     "start_time": "2020-05-12T13:43:24.682809Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/12/2020 13:43:24 - INFO - BERTQG.tokenization -   loading vocabulary file BERTQG/models/bert-base-uncased/vocab.txt\n",
      "05/12/2020 13:43:24 - INFO - BERTQG.modeling -   loading archive file BERTQG/models/bert-base-uncased\n",
      "05/12/2020 13:43:24 - INFO - BERTQG.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 3,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load BERTQG/models/lm_10k_QG/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/12/2020 13:43:26 - INFO - BERTQG.tokenization -   loading vocabulary file BERTQG/models/bert-base-uncased/vocab.txt\n",
      "05/12/2020 13:43:27 - INFO - BERTQG.modeling -   loading archive file BERTQG/models/bert-base-uncased\n",
      "05/12/2020 13:43:27 - INFO - BERTQG.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 3,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load BERTQG/models/uqa_10k_QG/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "bert_model = 'BERTQG/models/bert-base-uncased'\n",
    "lm_qg = 'BERTQG/models/lm_10k_QG/pytorch_model.bin'\n",
    "uqa_qg = 'BERTQG/models/uqa_10k_QG/pytorch_model.bin'\n",
    "#regul_model = \"./discri_model.bin\"# model path here\n",
    "QG = QuestionGeneration(bert_model, lm_qg, uqa_qg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T13:32:54.182065Z",
     "start_time": "2020-05-12T13:32:54.179849Z"
    }
   },
   "outputs": [],
   "source": [
    "context = \"The black bittern (Ixobrychus flavicollis) is a bittern of Old World origin, breeding in tropical Asia from Bangladesh, Pakistan, India, and Sri Lanka east to China, Indonesia, and Australia. It is mainly resident, but some northern birds migrate short distances.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T13:32:54.262053Z",
     "start_time": "2020-05-12T13:32:54.183332Z"
    }
   },
   "outputs": [],
   "source": [
    "ans = \"Pakistan\"\n",
    "ans_start = context.find(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T13:32:55.355147Z",
     "start_time": "2020-05-12T13:32:54.266403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm tokens ['is', 'did', 'are', 'can', 'do', 'does', '?', 'where', 'it', 'will']\n",
      "uqa_qi_probs ['is', 'are', 'can', 'breeding', 'in', 'else', 'do', 'was', 'to', 'and']\n",
      "updated lm tokens ['is', 'did', 'are', 'can', 'do', 'does', '?', 'will', 'was', 'the']\n",
      "qi_class 1\n",
      "################\n",
      "lm tokens ['it', 'the', '?', 'its', 'found', 'this', 'that', 'located', 'from', 'a']\n",
      "uqa_qi_probs ['the', 'breeding', 'in', 'your', 'this', 'an', 'a', 'bangladesh', 'tropical', 'that']\n",
      "updated uqa_qi_probs ['the', 'breeding', 'in', 'your', 'this', 'an', 'a', 'bangladesh', 'tropical', 'that']\n",
      "qi_class 0\n",
      "################\n",
      "lm tokens ['bitter', '?', 'black', 'name', 'world', 'red', 'common', 'largest', 'most', 'burma']\n",
      "uqa_qi_probs ['black', 'tropical', 'breeding', 'blue', 'white', 'african', 'red', 'negro', 'southern', 'south']\n",
      "updated lm tokens ['bitter', '?', 'black', 'name', 'world', 'red', 'common', 'largest', 'most', 'burma']\n",
      "qi_class 1\n",
      "################\n",
      "lm tokens ['##n', '##ns', '##nn', '##na', '##ne', '##m', '##nne', '##nb', '##nt', '##nia']\n",
      "uqa_qi_probs ['##n', '##ns', '##nh', '##ne', '##na', '##nst', '##s', '##no', '##ness', '##ng']\n",
      "updated uqa_qi_probs ['of', 'breeding', 'and', ',', 'in', '?', 'from', 'new', '-', '\"']\n",
      "qi_class 0\n",
      "################\n",
      "lm tokens ['india', '?', 'china', 'bangladesh', 'the', 'where', 'burma', '.', 'east', 'is']\n",
      "uqa_qi_probs ['tropical', 'breeding', 'bangladesh', 'the', 'black', 'new', 'southern', 'subtropical', 'south', 'african']\n",
      "updated uqa_qi_probs ['tropical', 'breeding', 'bangladesh', 'the', 'black', 'new', 'southern', 'subtropical', 'south', 'african']\n",
      "qi_class 0\n",
      "################\n",
      "lm tokens ['asia', 'india', 'africa', 'china', 'america', '?', '##asia', '.', 'and', 'asian']\n",
      "uqa_qi_probs ['asia', 'asian', 'africa', 'india', '##asia', 'east', 'and', 'south', 'asiatic', 'in']\n",
      "updated uqa_qi_probs ['asia', 'asian', 'africa', 'india', 'east', 'and', 'south', 'asiatic', 'in', 'of']\n",
      "qi_class 0\n",
      "################\n",
      "lm tokens ['?', 'where', '.', ',', 'to', 'from', 'is', 'and', ':', '(']\n",
      "uqa_qi_probs ['from', ',', 'breeding', 'and', '?', 'is', 'in', '(', 'to', 'between']\n",
      "updated lm tokens ['?', '.', ',', 'to', 'from', 'is', 'and', ':', '(', '[SEP]']\n",
      "qi_class 1\n",
      "################\n",
      "lm tokens ['[SEP]', ',', 'come', 'a', '.', '-', 'o', '?', ')', 'dr']\n",
      "uqa_qi_probs ['[SEP]', ',', ')', 'a', \"'\", 'the', '\"', 'in', 'from', 'is']\n",
      "updated uqa_qi_probs ['[SEP]', ',', ')', 'a', \"'\", 'the', '\"', 'in', 'from', 'is']\n",
      "qi_class 0\n",
      "################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'where is the bitter of tropical asia ?'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QG.generate_question(context, ans, ans_start, ['where'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
