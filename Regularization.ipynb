{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary size = 30K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Freq_Regularization():\n",
    "    def predict_class(self, list_token_classes):\n",
    "        '''\n",
    "        UQA class = 0\n",
    "        LM class = 1\n",
    "        '''\n",
    "        prob_lm = 1 - sum(list_token_classes)/len(list_token_classes)\n",
    "        if prob_lm > 0.5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T16:03:25.098138Z",
     "start_time": "2020-04-21T16:03:21.050179Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from BERTQG.token_generation import load_model, generate_token\n",
    "#from Regularization_module import Regularizer_Discriminator\n",
    "\n",
    "class QuestionGeneration():\n",
    "    def __init__(self, bert_model, lm_gq, uqa_qg, regularization=None):\n",
    "        self.lm_qg, _, _ = load_model(bert_model, lm_qg)\n",
    "        self.uqa_qg, self.tokenizer, self.device = load_model(bert_model, uqa_qg)\n",
    "        self.regularization = Freq_Regularization()\n",
    "        \n",
    "    def generate_question(self, context: str, ans: str, ans_start: int, list_question_tokens: list = []) -> str:\n",
    "        '''\n",
    "        Input:\n",
    "            - context\n",
    "            - ans\n",
    "            - ans_start\n",
    "            - list_question_tokens: the history of generated question tokens.\n",
    "            The form of list_question_tokens is a list of (q_token, class), where class is 0 for lm\n",
    "            token and 1 for uqa token\n",
    "        Output:\n",
    "            - The next generated question token\n",
    "        '''\n",
    "        # contains the tokens of the generated question\n",
    "        list_question_tokens = []\n",
    "        # contains the classes of each token of the gen. question. Same len as list_question_tokens\n",
    "        # 0 = LM, 1 = UQA\n",
    "        list_token_classes = []\n",
    "        qi = None\n",
    "        \n",
    "        # generation finished when [SEP] is created\n",
    "        while not self.finished_generation(qi):\n",
    "            question_text = \" \".join(list_question_tokens)\n",
    "            \n",
    "            # Generate the toknes and probs of the ith query token using the lm and uqa models\n",
    "            lm_qi_token, lm_qi_idx, lm_qi_probs = generate_token(lm_qg, self.tokenizer, self.device, context, question_text, ans, ans_start)\n",
    "            uqa_qi_token, uqa_qi_idx, uqa_qi_probs = generate_token(uqa_qg, self.tokenizer, self.device, context, question_text, ans, ans_start)\n",
    "            \n",
    "            # Get token class to use\n",
    "            qi_class = self.regularization.predict_class(list_token_classes)\n",
    "            # Get the predicted token\n",
    "            if qi_class == 1: # LM\n",
    "                qi = lm_qi_token\n",
    "            else: # UQA\n",
    "                qi = uqa_qi_token\n",
    "                \n",
    "            list_question_tokens.append(qi)\n",
    "            list_token_classes.append(qi_class)\n",
    "\n",
    "        return \" \".join(list_question_tokens)\n",
    "            \n",
    "    def finished_generation(self, question_token):\n",
    "        return question_token =='[SEP]'\n",
    "    \n",
    "    def __convert2squad(self, context: str, answer: str, ans_start: int,  question: str) -> dict:\n",
    "        '''\n",
    "        Create a SQuAD instance\n",
    "        Inputs:\n",
    "            - context: paragrah\n",
    "            - answer\n",
    "            - ans_start\n",
    "            - question: might not be the full question (we are generating questions token by token)\n",
    "        Returns:\n",
    "            - squad instance\n",
    "        '''\n",
    "        squad = {'data': [], 'version': '1.0'}\n",
    "        squad['data'].append({'paragraphs': [{'title': 'title', \n",
    "                                              'context': context,\n",
    "                                              'qas': [{'answers': [{'answer_start': ans_start, 'text': answer}],\n",
    "                                                       'question': question,\n",
    "                                                       'id': 0}]}]})\n",
    "        return squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T16:03:55.604246Z",
     "start_time": "2020-04-21T16:03:31.692383Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/28/2020 13:40:22 - INFO - BERTQG.tokenization -   loading vocabulary file BERTQG/models/bert-base-uncased/vocab.txt\n",
      "04/28/2020 13:40:22 - INFO - BERTQG.modeling -   loading archive file BERTQG/models/bert-base-uncased\n",
      "04/28/2020 13:40:22 - INFO - BERTQG.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 3,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load BERTQG/models/lm_10k_QG/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/28/2020 13:40:24 - INFO - BERTQG.tokenization -   loading vocabulary file BERTQG/models/bert-base-uncased/vocab.txt\n",
      "04/28/2020 13:40:24 - INFO - BERTQG.modeling -   loading archive file BERTQG/models/bert-base-uncased\n",
      "04/28/2020 13:40:24 - INFO - BERTQG.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 3,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load BERTQG/models/uqa_10k_QG/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "bert_model = 'BERTQG/models/bert-base-uncased'\n",
    "lm_qg = 'BERTQG/models/lm_10k_QG/pytorch_model.bin'\n",
    "uqa_qg = 'BERTQG/models/uqa_10k_QG/pytorch_model.bin'\n",
    "#regul_model = \"./discri_model.bin\"# model path here\n",
    "QG = QuestionGeneration(bert_model, lm_qg, uqa_qg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T08:29:17.876864Z",
     "start_time": "2020-04-21T08:29:16.182043Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/workspace/ml-workspace/emnlp_qg/datasets/squad_train_evenly_distributed.json', 'r') as f:\n",
    "    squad = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = squad['data'][0]['paragraphs'][0]['context']\n",
    "ans = squad['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['text']\n",
    "ans_start = squad['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['answer_start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-49bb3c889e7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mQG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_question\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-f2ca543af288>\u001b[0m in \u001b[0;36mgenerate_question\u001b[0;34m(self, context, ans, ans_start, list_question_tokens)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# Generate the toknes and probs of the ith query token using the lm and uqa models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mlm_qi_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_qi_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_qi_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_qg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0muqa_qi_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muqa_qi_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muqa_qi_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muqa_qg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/ml-workspace/emnlp_qg/regularization/UQA/BERTQG/token_generation.py\u001b[0m in \u001b[0;36mgenerate_token\u001b[0;34m(model, tokenizer, device, context, question_text, answer, answer_start)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0mlabel_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m     \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_pos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# vocab size (30522)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "QG.generate_question( context, ans, ans_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:33:44.379956Z",
     "start_time": "2020-04-18T05:33:42.298477Z"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import numpy as np\n",
    "word_url = \"http://svnweb.freebsd.org/csrg/share/dict/words?view=co&content-type=text/plain\"\n",
    "response = urllib.request.urlopen(word_url)\n",
    "long_txt = response.read().decode()\n",
    "VOCAB_IDX2_STR = np.array(long_txt.splitlines()).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:33:44.384517Z",
     "start_time": "2020-04-18T05:33:44.381340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25487, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_IDX2_STR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:39:34.541969Z",
     "start_time": "2020-04-18T05:39:34.535600Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:39:35.041424Z",
     "start_time": "2020-04-18T05:39:35.036434Z"
    }
   },
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:40:46.146027Z",
     "start_time": "2020-04-18T05:40:46.140537Z"
    }
   },
   "outputs": [],
   "source": [
    "qi_probs = softmax(torch.empty(VOCAB_IDX2_STR.shape[0]).normal_(mean=4,std=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:40:46.497031Z",
     "start_time": "2020-04-18T05:40:46.455058Z"
    }
   },
   "outputs": [],
   "source": [
    "qi = torch.argmax(qi_probs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T05:41:13.161422Z",
     "start_time": "2020-04-18T05:41:13.151075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Valparaiso'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_IDX2_STR[qi].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
